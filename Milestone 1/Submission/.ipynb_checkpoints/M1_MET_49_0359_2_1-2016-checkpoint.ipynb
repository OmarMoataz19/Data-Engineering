{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1 - EDA and Preprocessing data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Table Of Contents\n",
    "- [Extraction](#Extraction)\n",
    "- [EDA](#EDA)\n",
    "- [Cleaning](#Cleaning)\n",
    "- [Outliers](#Outliers)\n",
    "- [Feature-Engineering](#Feature)\n",
    "- [GPS](#GPS)\n",
    "- [Look Up Table](#LookUP)\n",
    "- [Parquet File & Export](#Parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Extraction\"></a>\n",
    "# 1 - Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataSet(dataSet_path):\n",
    "    return pd.read_csv(dataSet_path + \"green_tripdata_2016-01.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './green_tripdata_2016-01.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataSet_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mread_dataSet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataSet_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m, in \u001b[0;36mread_dataSet\u001b[1;34m(dataSet_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_dataSet\u001b[39m(dataSet_path):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataSet_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgreen_tripdata_2016-01.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\omarm\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\omarm\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    663\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    664\u001b[0m     dialect,\n\u001b[0;32m    665\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    675\u001b[0m )\n\u001b[0;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\omarm\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\users\\omarm\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\omarm\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1212\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\users\\omarm\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    793\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './green_tripdata_2016-01.csv'"
     ]
    }
   ],
   "source": [
    "dataSet_path = \"./\"\n",
    "df = read_dataSet(dataSet_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"EDA\"></a>\n",
    "# 2- EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_table = {}\n",
    "\n",
    "def update_lookup(mappings):\n",
    "    for feature, mapping in mappings.items():\n",
    "        lookup_table[feature] = mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Values\n",
    "unique_vals = pd.DataFrame(df.nunique(), columns=['Num of Unique Values'])\n",
    "display(unique_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "display(\"Shape of the Dataframe:\")\n",
    "display(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Info\n",
    "display(\"Info of the Dataframe:\")\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation\n",
    "display(\"Correlation of Features:\")\n",
    "correlations = df.corr()\n",
    "display(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description\n",
    "display(\"Description of Dataframe:\")\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x) # to avoid the scientific notation(to be more readable)\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights from that Data Description:\n",
    "\n",
    "### 1. **Passenger Count**:\n",
    "- Most trips have only 1 passenger (mean is close to 1, and median is 1).\n",
    "- The max value of 666 passengers seems highly unrealistic for a taxi and might be an error that needs investigation or correction.\n",
    "\n",
    "### 2. **Trip Distance**:\n",
    "- Average trip distance is 2.76 miles, but 50% of the trips are 1.8 miles or shorter.\n",
    "- The maximum trip distance of 360.5 miles is unusually long for a taxi ride within a city.\n",
    "\n",
    "### 3. **Fare Amount**:\n",
    "- The average fare is around $11.94.\n",
    "- There's a negative minimum fare (-492.80), which seems to be a data error or might represent refunds/adjustments.\n",
    "\n",
    "### 4. **Extra**:\n",
    "- Half of the rides seem to have an extra charge of 0.50, but there's a max value of 83, which seems quite high.\n",
    "\n",
    "### 5. **MTA Tax**:\n",
    "- The vast majority of rides have an MTA tax of 0.50, as indicated by the 25th, 50th, and 75th percentiles.\n",
    "\n",
    "### 6. **Tip Amount**:\n",
    "- The median tip is 0, suggesting that a significant number of rides have no tip. However, the average tip is $1.25, indicating that while many rides don't have tips, some have significant tip amounts.\n",
    "- A max tip of 400 dollars might be an outlier or a very generous tip.\n",
    "\n",
    "### 7. **Tolls Amount**:\n",
    "- The vast majority of rides don't have toll charges, but some do go up to 900 dollars, which is very high and might be an error.\n",
    "\n",
    "### 8. **Ehail Fee**:\n",
    "- All entries are missing for this column. It might be worth considering dropping it if it's not relevant for the analysis.\n",
    "\n",
    "### 9. **Improvement Surcharge**:\n",
    "- Most rides have an improvement surcharge of 0.30.\n",
    "\n",
    "### 10. **Total Amount**:\n",
    "- The average total amount is 14.42, but 50 percent of rides are 11.16 or cheaper.\n",
    "- Negative values might represent refunds or adjustments but need to be checked for data quality.\n",
    "\n",
    "### 11. **Congestion Surcharge**:\n",
    "- There are only 2 non-missing values, both of which are 0. This column might not be very informative if almost all its values are missing, therefore it could be worth dropping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_index_candidate(df):\n",
    "    potential_indices = []\n",
    "    for column in df.columns:\n",
    "        if df[column].nunique() == df.shape[0] and df[column].isnull().sum() == 0:\n",
    "            potential_indices.append(column)\n",
    "    if not potential_indices:\n",
    "        return \"No potential index candidates found. \"\n",
    "    else:\n",
    "        return potential_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index Candidate(s), if any\n",
    "display(check_index_candidate(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No feature has unique values for each entry and does not contain any missing values, therefore there are no candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "def draw_correlation_Heatmap(df):\n",
    "    plt.figure(figsize=(15,10))\n",
    "    sns.heatmap(correlations, cmap='coolwarm', annot=True, fmt='.2f')\n",
    "    plt.title('Correlation Heatmap')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_correlation_Heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Insights\n",
    "\n",
    "- There is a strong correlation between the trip distance both the total amount and the fare amount\n",
    "- There is a very strong correlation between the mta tax and the improvement surcharge\n",
    "- There is an medium correlation between the tip amount and the total amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(df, feature, bins='auto', title=None):\n",
    "    \"\"\"\n",
    "    Plots a histogram for the given feature.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=df, x=feature, bins=bins)\n",
    "    plt.title(title if title else f'Distribution of {feature}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_density(df, feature, title=None):\n",
    "    \"\"\"\n",
    "    Plots a density plot for the given feature.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.kdeplot(data=df, x=feature)\n",
    "    plt.title(title if title else f'Density of {feature}')\n",
    "    plt.show()\n",
    "\n",
    "def plot_boxplot(df, feature, title=None):\n",
    "    \"\"\"\n",
    "    Plots a boxplot for the given feature.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=df, x=feature)\n",
    "    plt.title(title if title else f'Boxplot of {feature}')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_countplot(df, feature, title=None):\n",
    "    \"\"\"\n",
    "    Plots a countplot for a categorical or discrete feature.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    sns.countplot(data=df, x=feature, order=df[feature].value_counts().index)\n",
    "    \n",
    "    plt.title(title if title else f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_scatter(df, x_feature, y_feature, title=None):\n",
    "    \"\"\"\n",
    "    Plots a scatter plot for the given features.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df, x=x_feature, y=y_feature)\n",
    "    plt.title(title if title else f'Relationship between {x_feature} and {y_feature}')\n",
    "    plt.xlabel(x_feature)\n",
    "    plt.ylabel(y_feature)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_line(data, title, xlabel, ylabel):\n",
    "    \"\"\"\n",
    "    Create a line chart using matplotlib.\n",
    "\n",
    "    Parameters:\n",
    "    - data: Series or DataFrame with the data to be plotted.\n",
    "    - title: Title of the line chart.\n",
    "    - xlabel: Label for the x-axis.\n",
    "    - ylabel: Label for the y-axis.\n",
    "\n",
    "    Returns:\n",
    "    - None (displays the chart).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        for column in data.columns:\n",
    "            plt.plot(data.index, data[column], label=column)\n",
    "    else:\n",
    "        plt.plot(data.index, data)\n",
    "    \n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) What's the distribution of trip distances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_distance = int(df['trip distance'].max())\n",
    "plot_histogram(df, 'trip distance', bins=range(0, max_distance + 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density(df, 'trip distance', title= \"Distribution of trip distance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display((df[\"trip distance\"]==0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"trip distance\"]==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The 2 graphs indicate that the distance is right skewed indicating that most trips are very short. Moreover in 20437 cases 0 distance was recorded which means that either the taximeter wasn't functioning or there could be instances where rides are started and stopped without a passenger, either mistakenly or intentionally, to meet daily trip targets or other reasons, or depending on the granularity of the recording system, very short trips (like those within a large complex or between adjacent buildings) the driver didn't open the taximeter giving a distance of zero which could be seen since the duration of most of those trips is very short."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) What's the Passenger Count Distrubtion?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_countplot(df, 'passenger count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"passenger count\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The Graph indicates that the majority of the trips have a single passenger, however the count of 666 passengers for 5 rides is highly unrealistic and likely represents data entry errors or system glitches. Such values should be further investigated and possibly cleaned or corrected. Moreover, counts of 7, 8, and 9 are extremely rare, with only 43, 27, and 12 rides respectively. These might represent special vehicles or possible data entry errors also.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) What's the most popular Payment Type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_countplot(df, 'payment type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"payment type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Customers predominantly prefer cash and credit card. Uknown, indicating possible data entry errors or instances where the payment method was not captured accurately. Moreover, for Dispute it could be due to service dissatisfaction. Furthermore, for the No charge it could be due to promotions and discounts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) What's the relationship between the total amount and the tip amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(df, 'total amount', 'tip amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Based on the correlation heatmap and the scatter graph, it could be concluded that the total amount and the tip amount are highly correlated, as the total amount increases it could be seen that the passengers were more generous and the tip amount increased. However for some high total amount values indicated in the scatter graphs it could be seen that the tip amount was so low (or zero) which could mean that there was a service dissatisfaction or dispute. Moreover, most trips had no tip."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) What's the distribution of Trip Fare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density(df,\"fare amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(df,\"fare amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display((df[\"fare amount\"]==0).sum())\n",
    "display((df[\"fare amount\"]<0).sum())\n",
    "display((df[\"fare amount\"]>0).sum())\n",
    "display((df[\"fare amount\"]<200).sum())\n",
    "df[df[\"fare amount\"]<0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"fare amount\"]==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "There are 2977 instances where the fare amount is negative. This could be due to data recording issues or actual refunds or cancelled trips since they have close to zero distance, but further investigation would be needed to pinpoint the exact cause. for the zero fare it could be due to promotions however the data is right skewed (positively skewed)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) How does the trip distance relate to the fare amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(df, 'trip distance', 'fare amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df[\"fare amount\"] >200) & (df[\"trip distance\"] == 0)]\n",
    "df[(df[\"fare amount\"] >0) & (df[\"trip distance\"] == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "There is a linear relation between the trip distance and the fare amount ( as the distance increases the fare amount increases), However there are a lot of values with zero distance but high fares which could indicate Data entry errors where the taxi might have traveled a certain distance, but due to technical or human error, it got recorded as 0, howeover this doesn't look like the case since the trip durations are very short meaning it could be due to cancellations fees. As for the negative fares it could be due to Cancellation Charges since the time of the trips are very short as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) How does the trip distance relate to the number of passengers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_line(df.groupby('passenger count')['trip distance'].mean(),'distance to passenger count', 'passenger Count', 'Trip Distance' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Clearly we can see that the trip distance doesn't relate to the passenger count, moreover we can see that there is an outlier in the passenger count again and the average distance is small so it has nothing to do with the passenger count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Cleaning\"></a>\n",
    "# 3 - Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidying up column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_column_names(df, rename_dict=None):\n",
    "    # Remove leading/trailing whitespaces, replace inner spaces with underscores, and lowercase\n",
    "    df.columns = df.columns.str.strip().str.replace(' ', '_').str.lower()\n",
    "        \n",
    "    # Rename columns if a dictionary is provided\n",
    "    if rename_dict:\n",
    "        # Convert the keys of rename_dict to match our cleaned column names\n",
    "        rename_dict = {key.lower().replace(' ', '_'): value for key, value in rename_dict.items()}\n",
    "        df = df.rename(columns=rename_dict)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_column_names(df, rename_dict={\"lpep pickup datetime\": \"pickup_datetime\", \"lpep dropoff datetime\":\"dropoff_datetime\",\"store and fwd flag\":\"store_and_fwd\",\"pu_location\":\"pickup_location\",\"do_location\":\"dropoff_location\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the type of the date features from object to datetime..\n",
    "df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing & Handling irrelevant data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.shape) # Duplicates are removed, the 7 extra tuples were removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duplicates were removed since they don't add any additional relevant information and they increase the computational costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "\n",
    "def null_percentage(df):\n",
    "    perc_null = df.isnull().sum() / len(df)*100\n",
    "    return perc_null[perc_null>0]\n",
    "\n",
    "display(null_percentage(df_copy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy.extra.value_counts())\n",
    "nan_count = df_copy[\"extra\"].isna().sum()\n",
    "print(f\"Number of NaN values in the 'extra' column: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_passenger_rows = df[df[\"passenger_count\"].isnull()]\n",
    "display(null_passenger_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_copy['passenger_count'].mode()[0])\n",
    "print(df_copy['passenger_count'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_copy[\"payment_type\"].unique()\n",
    "print(unique_values)\n",
    "null_payment_rows = df[df[\"payment_type\"].isnull()]\n",
    "display(null_payment_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reasoning behind each decision of handling missing data:\n",
    "- For the `ehail_fee and congestion_surcharge` columns, every entry is missing. While one option is to retain these columns and fill them with zeros, this approach is problematic for a couple of reasons. First, it's implausible that all 1,445,294 entries genuinely have no associated `ehail fee or congestion surcharge`. Second, filling these columns with zeros might incorrectly suggest that these fees were specifically documented as zero, potentially leading to misunderstandings. Given these concerns, it's more prudent and straightforward to remove these two columns to prevent potential misinterpretations. Since every entry is missing, it could be a systematic issue, potentially suggesting MNAR.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- For the `extra` feature, nearly half of its entries are absent. This absence likely indicates (MNAR) because it's plausible that many trips simply did not incur additional fees. Therefore, a logical step would be to fill these missing values with zeros, signaling that no extra fees were applied to those particular trips.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- For the `passenger count` feature, it has only 411 null tuples, where this absence is likely (MAR), we can't simply remove those tuples since we will lose other information inside those rows about other features and to address the 411 missing values in the `passenger_count` column, we employed statistical imputation using the mode (which is 1). This approach prevents the loss of valuable data from other features in those rows. Using the mode or median for imputation assumes that the values are missing at random and helps maintain the overall statistical properties of the dataset. Moreover, this feature isn't correlated with other features, thus statistical imputation is the way to go.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "- For the `payment type` feature, it has 56,504 entries with missing values. Given the nature of the data, it's probable that these values are missing at random (MAR). Instead of removing these rows and losing valuable information from other columns, a more appropriate approach is to fill these missing values. Notably, the dataset already contains a category labeled as uknown (which should be corrected to unknown). Imputing the missing values with this unknown category makes sense, as it reflects the uncertainty regarding the payment method for these specific trips. But before filling with unknown we should first fill with credit category if there is a tip amount, since we know that tip amounts are only there for credit type trips.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Data Handling Functions \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statistical_imputation(df, column, method='mean'):\n",
    "    \"\"\"\n",
    "    Imputes missing values in a DataFrame column using a statistical method.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with the data.\n",
    "    - column: The name of the column to impute.\n",
    "    - method: The statistical method for imputation ('mean', 'median', or 'mode').\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with imputed values.\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == 'mean':\n",
    "        imputed_value = df[column].mean()\n",
    "    elif method == 'median':\n",
    "        imputed_value = df[column].median()\n",
    "    elif method == 'mode':\n",
    "        imputed_value = df[column].mode()[0]  # mode() returns a Series, so we get the first entry\n",
    "    else:\n",
    "        raise ValueError(\"Method should be one of 'mean', 'median', or 'mode'.\")\n",
    "    \n",
    "    df[column].fillna(imputed_value, inplace=True)\n",
    "    update_lookup({column: {\"null/nan\": imputed_value}})\n",
    "    return df \n",
    "\n",
    "def fill_missing_with_zeros(df, feature):\n",
    "    \"\"\"\n",
    "    Fills missing values in the specified feature with zeros.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - feature: column name as a string\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with missing values filled in the specified feature.\n",
    "    \"\"\"\n",
    "    df[feature].fillna(0, inplace=True)\n",
    "    update_lookup({feature: {\"null/nan\": 0}})    \n",
    "    return df, {feature: {\"null/nan\": 0}}\n",
    "\n",
    "def impute_missing_with_category(df, feature, category):\n",
    "    \"\"\"\n",
    "    Fills missing values in the specified feature with the given category.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - feature: column name as a string\n",
    "    - category: the category to fill the missing values with\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with missing values filled in the specified feature.\n",
    "    \"\"\"\n",
    "    df[feature].fillna(category, inplace=True)\n",
    "    update_lookup({feature: {pd.NA: category}})\n",
    "\n",
    "    return df, {feature: {pd.NA: category}}\n",
    "\n",
    "def impute_specific_value(df, column, target_value, method='mean'):\n",
    "    \"\"\"\n",
    "    Imputes rows in a DataFrame column with a specific target value using a statistical method.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame with the data.\n",
    "    - column: The name of the column to impute.\n",
    "    - target_value: Rows with this specific value in the column will be imputed.\n",
    "    - method: The statistical method for imputation ('mean', 'median', or 'mode').\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with imputed values.\n",
    "    \"\"\"\n",
    "    if method == 'mean':\n",
    "        imputed_value = df.loc[df[column] != target_value, column].mean()\n",
    "    elif method == 'median':\n",
    "        imputed_value = df.loc[df[column] != target_value, column].median()\n",
    "    elif method == 'mode':\n",
    "        imputed_value = df.loc[df[column] != target_value, column].mode()[0]  # mode() returns a Series, so we get the first entry\n",
    "    else:\n",
    "        raise ValueError(\"Method should be one of 'mean', 'median', or 'mode'.\")\n",
    "    print(imputed_value)\n",
    "    df.loc[df[column] == target_value, column] = imputed_value\n",
    "    update_lookup({column: {target_value: imputed_value}})\n",
    "\n",
    "    return df, {column: {target_value: imputed_value}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle the ehail_fee and congestion_surcharge as stated above\n",
    "df_copy.drop(columns=['ehail_fee', 'congestion_surcharge'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle the passenger_count as stated above.\n",
    "statistical_imputation(df_copy, \"passenger_count\", method=\"mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle extra as stated above.\n",
    "fill_missing_with_zeros(df_copy, \"extra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handle payment type as stated above.\n",
    "df_copy.loc[df_copy['tip_amount'] != 0, 'payment_type'] = df_copy.loc[df_copy['tip_amount'] != 0, 'payment_type'].fillna('Credit card')\n",
    "df_copy[\"payment_type\"].replace(\"Uknown\", \"unknown\", inplace=True) \n",
    "#handle the payment type missing values by placing unknown in null positions, as stated above.\n",
    "impute_missing_with_category(df_copy, \"payment_type\", \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy.shape)\n",
    "unknown_payments = df_copy[df_copy[\"payment_type\"] == \"unknown\"]\n",
    "unique_values = df_copy[\"payment_type\"].unique()\n",
    "print(unique_values)\n",
    "display(unknown_payments)\n",
    "display(df_copy)\n",
    "nan_count = df_copy[\"passenger_count\"].isna().sum()\n",
    "print(f\"Number of NaN values in the 'passenger count' column: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_specific_value(df_copy, \"payment_type\",'unknown', method=\"mode\")\n",
    "unique_values = df_copy[\"payment_type\"].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy[\"payment_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is done since Unknown is also considered missing since it could be due to the fact that these values were infact empty and somebody imputed them using unknown to indicate that they were missing. so we handle them by imputing using the most famous payment type (mode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(null_percentage(df_copy)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This  indicates all missing values were removed and handled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As shown from the shape of the columns, both `ehail_fee and congestion_surcharge` were dropped. Moreover, the `payment_type` was filled by unknown and replaced it from uknown to unknown. Furthermore, missing values in `extra` were filled with zeros as shown in the previous display. Finally, the `passenger_count` was statistically imputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorrect & Inconsistent Data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_column_for_multiple_values(df, col_to_adjust, adjustment_col, valid_values):\n",
    "    \"\"\"\n",
    "    Adjusts the adjustment_col based on invalid values in col_to_adjust. Sets invalid values in col_to_adjust to 0.\n",
    "    \n",
    "    :param df: DataFrame\n",
    "    :param col_to_adjust: Name of the column to check and adjust.\n",
    "    :param adjustment_col: Name of the column to decrement by the value in col_to_adjust if invalid.\n",
    "    :param valid_values: List of valid values for col_to_adjust.\n",
    "    :return: Adjusted DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the column to float for easy comparison\n",
    "    df[col_to_adjust] = df[col_to_adjust].astype(float)\n",
    "\n",
    "    # Create a mask for rows with invalid col_to_adjust values\n",
    "    mask = ~df[col_to_adjust].isin(valid_values)\n",
    "\n",
    "    # Adjust the adjustment_col based on the mask\n",
    "    df.loc[mask, adjustment_col] -= df.loc[mask, col_to_adjust]\n",
    "\n",
    "    # Set the invalid col_to_adjust values to 0\n",
    "    df.loc[mask, col_to_adjust] = 0.0\n",
    "\n",
    "    return df\n",
    "\n",
    "def remove_rows_with_value(df, column_name, value_to_remove):\n",
    "    \"\"\"\n",
    "    Removes rows from a DataFrame based on a specific value in a specified column.\n",
    "\n",
    "    :param df: DataFrame from which rows will be removed.\n",
    "    :param column_name: Name of the column to be checked.\n",
    "    :param value_to_remove: Value to be removed from the DataFrame.\n",
    "    :return: DataFrame with rows removed.\n",
    "    \"\"\"\n",
    "    return df[df[column_name] != value_to_remove]\n",
    "\n",
    "def remove_rows_with_multiple_conditions(df, conditions):\n",
    "    \"\"\"\n",
    "    Removes rows from a DataFrame based on multiple conditions specified for different columns.\n",
    "    Rows are removed if all specified conditions are met simultaneously.\n",
    "\n",
    "    :param df: DataFrame from which rows will be removed.\n",
    "    :param conditions: List of tuples, each containing a feature name and the value to be checked.\n",
    "    :return: DataFrame with rows removed.\n",
    "    \"\"\"\n",
    "    mask = pd.Series([True] * len(df))\n",
    "    for feature, value in conditions:\n",
    "        mask = mask & (df[feature] == value)\n",
    "    return df[~mask]\n",
    "\n",
    "def adjust_column_value_based_on_condition(df, condition_column, condition_value, target_column, desired_value):\n",
    "    \"\"\"\n",
    "    Adjusts the value of the target column based on a condition in another column.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame.\n",
    "    - condition_column: The column name where we check the condition.\n",
    "    - condition_value: The value to check against in the condition column.\n",
    "    - target_column: The column name where we want to adjust the value.\n",
    "    - desired_value: The value to set in the target column if the condition is met.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame with adjusted values.\n",
    "    \"\"\"\n",
    "    condition = (df[condition_column] == condition_value) & (df[target_column] != desired_value)\n",
    "    df.loc[condition, target_column] = desired_value\n",
    "    return df\n",
    "\n",
    "def adjust_features_based_on_conditions(df, conditions, adjustments):\n",
    "    \"\"\"\n",
    "    Adjusts values of specified features in a DataFrame based on given conditions for other features.\n",
    "\n",
    "    :param df: Pandas DataFrame.\n",
    "    :param conditions: List of tuples, each containing a column name, a comparison operator,\n",
    "                      and a value for the condition (e.g., [('vendor', '==', 'VeriFone Inc.'), ('total_amount', '>', 50)]).\n",
    "    :param adjustments: Dictionary where keys are feature names to be adjusted, and values are new values.\n",
    "    :return: Pandas DataFrame with adjusted feature values.\n",
    "    \"\"\"\n",
    "    mask = None\n",
    "    for condition_column, operator, condition_value in conditions:\n",
    "        condition_mask = df.eval(f\"{condition_column} {operator} @condition_value\", engine='python', local_dict={'condition_value': condition_value})\n",
    "        mask = condition_mask if mask is None else mask & condition_mask\n",
    "\n",
    "    if mask is not None:\n",
    "        for feature, new_value in adjustments.items():\n",
    "            df.loc[mask, feature] = new_value\n",
    "\n",
    "    return df\n",
    "\n",
    "def count_mismatch_rows(df, columns_to_sum, total_column, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Count the rows where the sum of specified columns is not equal to the total column.\n",
    "\n",
    "    Parameters:\n",
    "    - df: The DataFrame.\n",
    "    - columns_to_sum: List of column names to be summed.\n",
    "    - total_column: The column name of the total value to be compared against.\n",
    "\n",
    "    Returns:\n",
    "    - Count of rows where the sum of specified columns is not equal to the total column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the sum for each row\n",
    "    row_sums = df[columns_to_sum].sum(axis=1)\n",
    "    \n",
    "    # Find rows where the sum isn't equal to the total\n",
    "    mismatch_rows = df[abs(row_sums - df[total_column]) > tolerance]\n",
    "    \n",
    "    return len(mismatch_rows)\n",
    "\n",
    "def remove_mismatched_rows(df, col_list, total_col, tolerance=1e-6):\n",
    "    \"\"\" \n",
    "    Removes rows where the sum of values in col_list doesn't match the value in total_col within a given tolerance.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame\n",
    "    - col_list: List of column names to sum\n",
    "    - total_col: Column name to compare the sum against\n",
    "    - tolerance: Maximum difference allowed between the summed value and total_col value (default is 0.01)\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with mismatched rows removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the row-wise sum of columns in col_list\n",
    "    df['calculated_sum'] = df[col_list].sum(axis=1)\n",
    "    \n",
    "    # Determine rows where the difference between calculated_sum and total_col is greater than the tolerance\n",
    "    mask = (df['calculated_sum'] - df[total_col]).abs() > tolerance\n",
    "    \n",
    "    # Remove those rows\n",
    "    filtered_df = df[~mask].drop(columns=['calculated_sum'])\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "def replace_values(df, feature, old_value, new_value):\n",
    "    \"\"\"\n",
    "    Replaces occurrences of old_value with new_value in the specified feature of the DataFrame.\n",
    "\n",
    "    :param df: Pandas DataFrame.\n",
    "    :param feature: Name of the feature/column where values should be replaced.\n",
    "    :param old_value: Value to be replaced.\n",
    "    :param new_value: Value to replace old_value with.\n",
    "    :return: Pandas DataFrame with replaced values.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    df_copy[feature] = df_copy[feature].replace(old_value, new_value)\n",
    "    \n",
    "    # Prepare the update details for the lookup table\n",
    "    update_details = {feature: {old_value: new_value}}\n",
    "    update_lookup(update_details)\n",
    "    return df_copy\n",
    "\n",
    "def filter_records_by_date_range(df, column_name, lower_bound=None, upper_bound=None):\n",
    "    \"\"\"\n",
    "    Filter DataFrame rows based on a datetime column's values being within a specified date range.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The dataframe.\n",
    "    - column_name: Name of the datetime column to be filtered.\n",
    "    - lower_bound: The lower bound of the date range (inclusive). If None, no lower filtering is applied.\n",
    "    - upper_bound: The upper bound of the date range (inclusive). If None, no upper filtering is applied.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with rows filtered based on specified date range.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert the bounds to datetime\n",
    "    if lower_bound is not None:\n",
    "        lower_bound = pd.to_datetime(lower_bound).normalize()\n",
    "    \n",
    "    if upper_bound is not None:\n",
    "        upper_bound = pd.to_datetime(upper_bound).normalize() + pd.DateOffset(days=1) - pd.Timedelta(seconds=1)\n",
    "\n",
    "    # Apply filtering\n",
    "    if lower_bound:\n",
    "        df = df[df[column_name] >= lower_bound]\n",
    "    \n",
    "    if upper_bound:\n",
    "        df = df[df[column_name] <= upper_bound]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"passenger count\" column to integer\n",
    "df_copy[\"passenger_count\"] = df_copy[\"passenger_count\"].astype(int)\n",
    "print(df_copy.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting the `passenger_count` to an integer type aligns better with the practical reality that taxis can only carry whole numbers of passengers; having fractional passenger counts is not realistic. We also removed outliers like 666 from the dataset, given that it's implausible for a taxi to hold such a high number of passengers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_copy['extra'].unique()\n",
    "formatted_values = [f\"{value:.2f}\" for value in unique_values]\n",
    "print(formatted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the -1 is an incorrect value\n",
    "filtered_count = df[(df['extra'] == -1)].shape[0]\n",
    "display(filtered_count)\n",
    "filtered_count = df[(df['extra'] == -1)& (df['total_amount'] < 0)].shape[0]\n",
    "display(filtered_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the -0.5 is an incorrect value\n",
    "filtered_count = df[(df['extra'] == -0.5)].shape[0]\n",
    "display(filtered_count)\n",
    "filtered_count = df[(df['extra'] == -0.5)& (df['total_amount'] < 0)].shape[0]\n",
    "display(filtered_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_count = df[(df['extra'] == 0.04)].shape[0]\n",
    "display(filtered_count)\n",
    "filtered_count = df[(df['extra'] == 0.02)].shape[0]\n",
    "display(filtered_count)\n",
    "filtered_count = df[(df['extra'] == 0.25)].shape[0]\n",
    "display(filtered_count)\n",
    "filtered_count = df[(df['extra'] == 2)].shape[0]\n",
    "display(filtered_count)\n",
    "filtered_count = df[(df['extra'] == 4.5)].shape[0]\n",
    "display(filtered_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From our analysis, negative values in the `extra` column seem to suggest trip refunds, as corroborated by corresponding negative totals. While there is a significant number of such records, values like 0.04, 0.02, 0.25, 2, and 4.5 appear infrequently, implying they might be errors. The column's description only mentions 0.5 and 1 as valid values, further supporting this assumption. Additionally, the value 83 stands out as a clear outlierit's unrealistic in this context. Consequently, to maintain consistency in our dataset, it's prudent to reset these irregular `extra` values to 0 and adjust the `total_amount` column by the respective amounts. For us to keep information from other features rather than just dropping those rows. Moreover the negative values will be analyzed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df['extra'] == 0.25].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the incorrect values in the extra column..\n",
    "valid_values = [0.5, 0.0, -0.5, 1, -1, 83.00]\n",
    "df_copy = adjust_column_for_multiple_values(df_copy, 'extra', 'total_amount', valid_values)\n",
    "unique_values = df_copy['extra'].unique()\n",
    "formatted_values = [f\"{value:.2f}\" for value in unique_values]\n",
    "print(formatted_values)\n",
    "\n",
    "\n",
    "# The outlier (83) will be handled in the following section(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same Rational will apply for the mta tax and the surcharge features.(This is based on the column descriptions)\n",
    "unique_values = df_copy['mta_tax'].unique()\n",
    "formatted_values = [f\"{value:.2f}\" for value in unique_values]\n",
    "print(formatted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the -0.5 is an incorrect value\n",
    "filtered_count = df_copy[(df_copy['mta_tax'] == -0.5)].shape[0]\n",
    "display(filtered_count)\n",
    "filtered_count = df_copy[(df_copy['mta_tax'] == -0.5)& (df_copy['total_amount'] < 0)].shape[0]\n",
    "display(filtered_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_count = df_copy[(df_copy['mta_tax'] == 2.5)].shape[0]\n",
    "display(filtered_count)\n",
    "filtered_count = df_copy[(df_copy['mta_tax'] == 3)].shape[0]\n",
    "display(filtered_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From our analysis, negative values in the `mta_tax` column seem to suggest trip refunds, as corroborated by corresponding negative totals. While there is a significant number of such records, values like 2.5 and 3 appear infrequently, implying they might be errors. The column's description only mentions 0.5  or possible 0 as valid values. Consequently, to maintain consistency in our dataset, it's prudent to reset these irregular `mta_tax` values to 0 and adjust the `total_amount` column by the respective amounts. For us to keep information from other features rather than just dropping those rows. Moreover the negative values will be analyzed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the Incorrect Values\n",
    "valid_values = [0.5, 0.0, -0.5]\n",
    "df_copy = adjust_column_for_multiple_values(df_copy, 'mta_tax', 'total_amount', valid_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_copy['mta_tax'].unique()\n",
    "formatted_values = [f\"{value:.2f}\" for value in unique_values]\n",
    "print(formatted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_copy['improvement_surcharge'].unique()\n",
    "formatted_values = [f\"{value:.2f}\" for value in unique_values]\n",
    "print(formatted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking whether the -0.3 is an incorrect value\n",
    "filtered_count = df_copy[(df_copy['improvement_surcharge'] == -0.3)].shape[0]\n",
    "display(filtered_count)\n",
    "filtered_count = df_copy[(df_copy['improvement_surcharge'] == -0.3)& (df_copy['total_amount'] < 0)].shape[0]\n",
    "display(filtered_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_count = df_copy[(df_copy['improvement_surcharge'] == 0.97)].shape[0]\n",
    "display(filtered_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From our analysis, negative values in the `improvement_surcharge` column seem to suggest trip refunds, as corroborated by corresponding negative totals. While there is a significant number of such records, other values rather than 0.3 , 0, -0.3 appear infrequently, implying they might be errors. The column's description only mentions 0.3 or possible 0 as valid values. Consequently, to maintain consistency in our dataset, it's prudent to reset these irregular `improvement_surcharge` values to 0 and adjust the `total_amount` column by the respective amounts. For us to keep information from other features rather than just dropping those rows. Moreover the negative values will be analyzed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the Incorrect Values\n",
    "valid_values = [0.3, 0.0, -0.3]\n",
    "df_copy = adjust_column_for_multiple_values(df_copy, 'improvement_surcharge', 'total_amount', valid_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_copy['improvement_surcharge'].unique()\n",
    "formatted_values = [f\"{value:.2f}\" for value in unique_values]\n",
    "print(formatted_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_copy['trip_type'].unique()\n",
    "display(unique_values)\n",
    "filtered_count = df_copy[(df_copy['trip_type'] == \"Unknown\")].shape[0]\n",
    "display(filtered_count)\n",
    "filtered_count = df_copy[(df_copy['improvement_surcharge'] == 0.3) & (df_copy['trip_type'] == \"Dispatch\")].shape[0]\n",
    "filtered_count1 = df_copy[(df_copy['improvement_surcharge'] == -0.3) & (df_copy['trip_type'] == \"Dispatch\")].shape[0]\n",
    "display(filtered_count)\n",
    "display(filtered_count1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Only two rows in our dataset have an 'Unknown' trip type. Given the minimal presence of this category and the fact that our dataset primarily consists of two distinct trip types for street-hail dispatch, it's prudent to remove these 'Unknown' entries. This way, we prevent introducing an unnecessary category. Moreover since we have 157 rows with dispatch type and surcharge is only assessed in street hail trips, furthemore we know that the driver could alter the type. Then this must be an error and we should handle it by changing those trips to street hail since there is a surcharge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the irrelevant data \n",
    "df_copy = remove_rows_with_value(df_copy, 'trip_type', 'Unknown')\n",
    "unique_values = df_copy['trip_type'].unique()\n",
    "display(unique_values)\n",
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = adjust_column_value_based_on_condition(df_copy, 'improvement_surcharge', 0.3, 'trip_type', 'Street-hail')\n",
    "df_copy = adjust_column_value_based_on_condition(df_copy, 'improvement_surcharge', -0.3, 'trip_type', 'Street-hail')\n",
    "unique_values = df_copy['trip_type'].unique()\n",
    "display(unique_values)\n",
    "filtered_count = df_copy[(df_copy['improvement_surcharge'] == 0.3) & (df_copy['trip_type'] == \"Dispatch\")].shape[0]\n",
    "display(filtered_count)\n",
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_count = df_copy[(df_copy['tip_amount'] > 0) & (df_copy['payment_type'] != \"Credit card\")].shape[0]\n",
    "display(filtered_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we know that the tip amount is only there if the payment type is Credit Card we should clean the data to meet that criteria by transforming any row that has tip amount but not credit card payment to have a credit card payment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition to check if tip amount is greater than 0 and payment type isn't 'credit card'\n",
    "condition = (df_copy['tip_amount'] > 0) & (df_copy['payment_type'] != \"Credit card\")\n",
    "    \n",
    "# Adjust the payment type to 'credit card' where the condition is met\n",
    "df_copy.loc[condition, 'payment_type'] = 'Credit card'\n",
    "    \n",
    "filtered_count = df_copy[(df_copy['tip_amount'] > 0) & (df_copy['payment_type'] != \"Credit card\")].shape[0]\n",
    "display(filtered_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_mismatch = count_mismatch_rows(df_copy, ['extra', 'mta_tax', 'tolls_amount','improvement_surcharge','fare_amount','tip_amount'], 'total_amount')\n",
    "display(count_mismatch)\n",
    "display(df_copy.shape)\n",
    "df_copy = remove_mismatched_rows(df_copy,['extra', 'mta_tax', 'tolls_amount','improvement_surcharge','fare_amount','tip_amount'], 'total_amount')\n",
    "display(df_copy.shape)\n",
    "count_mismatch = count_mismatch_rows(df_copy, ['extra', 'mta_tax', 'tolls_amount','improvement_surcharge','fare_amount','tip_amount'], 'total_amount')\n",
    "display(count_mismatch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We've eliminated rows where the total cost doesn't align with the sum of its components. This discrepancy is likely due to data entry errors. Given that the number of affected records is minimal and we can't pinpoint the exact source of the inconsistency, we chose to remove these rows. This approach ensures we avoid making assumptions and prevents any potential inaccuracies, especially when dealing with financial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter rows with negative total amount\n",
    "negative_total_amount_rows = df[df['total_amount'] < 0]\n",
    "\n",
    "# Display the 'vendor' column for these rows\n",
    "print(negative_total_amount_rows['vendor'])\n",
    "plot_histogram(negative_total_amount_rows, 'vendor', title='Distribution of Vendors for Negative Total Amounts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "negative_total_amount_rows = df_copy[df_copy['total_amount'] < 0]\n",
    "\n",
    "# Display the 'vendor' column for these rows\n",
    "display(negative_total_amount_rows.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(negative_total_amount_rows, 'total_amount')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplot(df_copy,'total_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_positive(df, features):\n",
    "    \"\"\"\n",
    "    Convert specified features' values from negative to positive in the given DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - features: List of feature column names to be converted.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with specified features' negative values turned to positive.\n",
    "    \"\"\"\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].apply(lambda x: abs(x) if x < 0 else x)\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_matching_trips(df):\n",
    "    # Filter rows based on given conditions\n",
    "    filtered_df = df[(df[\"total_amount\"] < 0) & \n",
    "                     (df[\"fare_amount\"] < 0) & \n",
    "                     (df[\"extra\"] <= 0) & \n",
    "                     (df['tolls_amount'] <= 0) & \n",
    "                     (df[\"mta_tax\"] <= 0) & \n",
    "                     (df[\"improvement_surcharge\"] <= 0) & \n",
    "                     (df['tip_amount'] <= 0)]\n",
    "    \n",
    "    # Extract values from 'filtered_df' to be matched against the original df\n",
    "    pickup_times = filtered_df['pickup_datetime']\n",
    "    dropoff_times = filtered_df['dropoff_datetime']\n",
    "    pu_locations = filtered_df['pickup_location']\n",
    "    do_locations = filtered_df['dropoff_location']\n",
    "\n",
    "    # Find rows in original df with matching 'pickup_datetime', 'dropoff_datetime', \n",
    "    # 'pu_location', and 'do_location' against 'filtered_df'\n",
    "    matching_trips = df[df['pickup_datetime'].isin(pickup_times) & \n",
    "                        df['dropoff_datetime'].isin(dropoff_times) & \n",
    "                        df['pickup_location'].isin(pu_locations) & \n",
    "                        df['dropoff_location'].isin(do_locations)]\n",
    "    return matching_trips\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = find_matching_trips(df_copy)\n",
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#df_copy = convert_to_positive(df_copy, ['extra', 'mta_tax', 'tolls_amount','improvement_surcharge','fare_amount','tip_amount', 'total_amount'])\n",
    "count_mismatch = count_mismatch_rows(df_copy, ['extra', 'mta_tax', 'tolls_amount','improvement_surcharge','fare_amount','tip_amount'], 'total_amount')\n",
    "display(count_mismatch)      \n",
    "display(df_copy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "negative_total_amount_rows = df_copy[df_copy['total_amount'] < 0]\n",
    "display(negative_total_amount_rows.shape)\n",
    "df_copy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(negative_total_amount_rows, 'payment_type')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we can see that the vendor Verifone was the only vendor with negative values indicating that they might give refunds and not a system error with multiple vendors offering refunds, moreover every negative row had an equivelent positive row. Futhermore most of those trips had no charge or dispute indicating that those weren't a mistake. Thus we leave the negatives as is since they indicate a refund in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_distance_rows = df_copy[(df_copy['trip_distance'] == 0) & (df_copy['total_amount'] ==0)]\n",
    "display(zero_distance_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_pairs = [('total_amount', 0), ('trip_distance', 0)]\n",
    "df_copy = remove_rows_with_multiple_conditions(df_copy, remove_pairs)\n",
    "display(df_copy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since These rows have total amount of zero and trip distance of 0 and clearly we can see the duration is very low we simply remove those rows since they are irrelevant to keep. (There is no trip) just empty data, why keep it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0) ]\n",
    "display(zero_total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the 'vendor' column for these rows\n",
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0) ]\n",
    "plot_histogram(zero_total_rows, 'vendor', title='Distribution of Vendors for 0 Total Amounts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(zero_total_rows, 'rate_type', title='Distribution of Vendors for 0 Total Amounts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0)  & (df_copy['vendor']== \"Creative Mobile Technologies, LLC\")  ]\n",
    "display(zero_total_rows.shape)\n",
    "\n",
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0) & (df_copy['vendor'] == \"Creative Mobile Technologies, LLC\") & (df_copy['rate_type'] == \"Negotiated fare\") ]\n",
    "display(zero_total_rows.shape)\n",
    "\n",
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0) & (df_copy['payment_type'] == \"Dispute\") & (df_copy['vendor'] == \"Creative Mobile Technologies, LLC\")]\n",
    "display(zero_total_rows.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0)  & (df_copy['vendor']== \"Creative Mobile Technologies, LLC\")  ]\n",
    "\n",
    "plot_histogram(zero_total_rows , 'payment_type' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0)  & (df_copy['vendor'] != \"Creative Mobile Technologies, LLC\")  ]\n",
    "display(zero_total_rows.shape)\n",
    "\n",
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0) & (df_copy['vendor'] != \"Creative Mobile Technologies, LLC\") & (df_copy['rate_type'] == \"Standard rate\") ]\n",
    "display(zero_total_rows.shape)\n",
    "\n",
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0) & (df_copy['payment_type'] == \"Cash\") & (df_copy['vendor'] != \"Creative Mobile Technologies, LLC\")]\n",
    "display(zero_total_rows.shape)\n",
    "\n",
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0) & (df_copy['payment_type'] == \"Credit card\") & (df_copy['vendor'] != \"Creative Mobile Technologies, LLC\")]\n",
    "display(zero_total_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0)  & (df_copy['trip_type'] == \"Dispatch\") & (df_copy['rate_type'] == \"Negotiated fare\") & (df_copy['vendor'] == \"Creative Mobile Technologies, LLC\") ]\n",
    "display(zero_total_rows.shape)\n",
    "\n",
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0)  & (df_copy['trip_type'] == \"Dispatch\") &  (df_copy['vendor'] != \"Creative Mobile Technologies, LLC\") ]\n",
    "display(zero_total_rows.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Verifone it's clear that all trips were either cash or credit, moreover one can see that only 1 row was of negotiated rate. Furtheremore, only 2 were marked as dispatch(ordered throught the phone, only way to give a refund/discount).Therefore, these rows (52) could be a mistake and should be removed since we know that verifone earlier gives refunds as negative values not 0s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For The other vendor (Creative LLC ) it has 2813 Dispatch rides with 0 total, and 2807 Negotiated. Thus, those could be refunds and should be marked as no charge in the payment_type. However 6 of them are standard fare which also should be negotiated fare indicating that this ride was a refund or a discount. For more details, we can think of it as if the driver had a bad experience reported that ride to the vendor they offered him a discount/refund for that ride or the next ride as some sort of compensation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "conditions = [('vendor', '==', 'Creative Mobile Technologies, LLC'), ('total_amount', '==', 0)]\n",
    "adjustments = {'rate_type': 'Negotiated fare', 'payment_type': 'No charge','trip_type':'Dispatch'}\n",
    "df_copy = adjust_features_based_on_conditions(df_copy, conditions, adjustments)\n",
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0) ]\n",
    "display(zero_total_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_total_rows = df_copy[(df_copy['total_amount'] == 0) & (df_copy['vendor'] == \"Creative Mobile Technologies, LLC\") & (df_copy['rate_type'] == \"Negotiated fare\") & (df_copy['trip_type'] == \"Dispatch\") &(df_copy['payment_type'] == \"No charge\")]\n",
    "display(zero_total_rows.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove those of Venifone\n",
    "remove_pairs = [('total_amount', 0), ('vendor',\"VeriFone Inc.\" )]\n",
    "df_copy = remove_rows_with_multiple_conditions(df_copy, remove_pairs)\n",
    "display(df_copy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_values = df_copy['rate_type'].unique()\n",
    "display(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we know there is a rate type for group ride we need to check if all group rides have more than 1 passenger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_rows = df_copy[(df_copy['rate_type'] == \"Group ride\") & (df_copy['passenger_count'] == 1) ]\n",
    "display(incorrect_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### since the passenger count is a driver entered value, human errors could occur which could be the case here, thus we impute those values by 2 since it's the second most common count and it falls under the category group ride."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [('rate_type', '==', 'Group ride'), ('passenger_count', '==', 1)]\n",
    "adjustments = {'passenger_count': 2}\n",
    "df_copy = adjust_features_based_on_conditions(df_copy, conditions, adjustments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_rows = df_copy[(df_copy['rate_type'] == \"Group ride\") & (df_copy['passenger_count'] == 1) ]\n",
    "display(incorrect_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "incorrect_rows = df_copy[(df_copy['trip_distance'] == 0)  ]\n",
    "display(incorrect_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the trips with distance 0 we can't simply get rid of them since they have a total amount, moreover it could be due to a misfunctioning taximeter, thus we indicate this by adding -1 , since we can't impute using the median since there is a strong correlation between the distance and the total amount and this will give false information, thus we indicate that the taximeter wasn't working by adding -1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_copy = replace_values(df_copy, 'trip_distance', 0, -1)\n",
    "incorrect_rows = df_copy[(df_copy['trip_distance'] == 0)  ]\n",
    "display(incorrect_rows)\n",
    "new_rows = df_copy[(df_copy['trip_distance'] == -1)]\n",
    "display(new_rows.shape) # same number as the 0 before imputing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rows outside the dataset range since they are irrelevant, and the dataset is only concerned with January 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2016-01-01\"\n",
    "end_date = \"2016-01-31\"\n",
    "df_copy = filter_records_by_date_range(df_copy, \"pickup_datetime\", start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Outliers\"></a>\n",
    "## Observing & Handling outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_values(df, feature, unwanted_value):\n",
    "    \"\"\"\n",
    "    Removes rows with unwanted values from the given feature column.\n",
    "    \"\"\"\n",
    "    return df[df[feature] != unwanted_value]\n",
    "\n",
    "def fix_unwanted_values(df, feature, unwanted_value, replacement_value):\n",
    "    \"\"\"\n",
    "    Fixes unwanted values in the given feature column.\n",
    "\n",
    "    Args:\n",
    "        df: The DataFrame to fix the values in.\n",
    "        feature: The name of the feature column to fix the values in.\n",
    "        unwanted_value: The value to fix.\n",
    "        replacement_value: The value to replace the unwanted value with.\n",
    "\n",
    "    Returns:\n",
    "        The DataFrame with the unwanted values fixed.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the DataFrame to avoid modifying the original DataFrame.\n",
    "    fixed_df = df.copy()\n",
    "\n",
    "    # Replace the unwanted value with the replacement value in the specified feature column.\n",
    "    fixed_df.loc[fixed_df[feature] == unwanted_value, feature] = replacement_value\n",
    "\n",
    "    # Return the DataFrame with the unwanted values fixed.\n",
    "    return fixed_df\n",
    "\n",
    "def impute_outliers_with_mean(df, feature, total_column=None, threshold=10):\n",
    "    \"\"\"\n",
    "    Impute outliers in a feature using the mean of positive values within a given threshold range.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The data frame.\n",
    "    - feature (str): The column with outliers.\n",
    "    - total_column (str, optional): The total amount column to adjust based on imputed values.\n",
    "    - threshold (float, optional): The threshold value to determine outliers.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The modified data frame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the mean of positive values between 0 and threshold\n",
    "    mean_val = df[(df[feature] > 0) & (df[feature] < threshold)][feature].mean()\n",
    "    \n",
    "    # Create a mask for rows that are outliers (values below -threshold or above threshold)\n",
    "    outliers_mask = (df[feature] < -threshold) | (df[feature] > threshold)\n",
    "    \n",
    "    if total_column:\n",
    "        # Adjust the total column based on the difference between the outlier and mean value\n",
    "        df.loc[outliers_mask, total_column] = df.loc[outliers_mask, total_column] - df.loc[outliers_mask, feature] + mean_val\n",
    "    \n",
    "    # Impute the outliers with the mean value\n",
    "    df.loc[outliers_mask, feature] = mean_val\n",
    "    \n",
    "    return df\n",
    "\n",
    "def impute_outliers_with_mean_iqr(df, feature, total_column=None):\n",
    "    \"\"\"\n",
    "    Impute outliers in a feature using the mean of values within the IQR range.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The data frame.\n",
    "    - feature (str): The column with outliers.\n",
    "    - total_column (str): The total amount column to adjust based on imputed values (optional).\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The modified data frame.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate the IQR and boundaries\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Create a mask for rows that are outliers\n",
    "    outliers_mask = (df[feature] < lower_bound) | (df[feature] > upper_bound)\n",
    "    \n",
    "    # Calculate the mean of values within the IQR range\n",
    "    mean_val = df[~outliers_mask][feature].mean()\n",
    "    \n",
    "    if total_column:\n",
    "        # Adjust the total column based on the difference between the outlier and mean value\n",
    "        df.loc[outliers_mask, total_column] = df.loc[outliers_mask, total_column] - df.loc[outliers_mask, feature] + mean_val\n",
    "    \n",
    "    # Impute the outliers with the mean value\n",
    "    df.loc[outliers_mask, feature] = mean_val\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_copy[\"passenger_count\"].unique()\n",
    "print(unique_values)\n",
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix out rows where passenger count is 666\n",
    "df_copy = fix_unwanted_values(df_copy, \"passenger_count\", 666, 6)\n",
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_copy[\"passenger_count\"].unique()\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We simply fixed the rows with passenger count(s) with value 666 since this is clearly an outlier since it's not feasible to have a ride with 666 passengers, which indicates that this is a data entry error since the user might have pressed 6 2 additional times and he meant to have 6 only once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_copy[\"extra\"].unique()\n",
    "print(unique_values)\n",
    "display (df_copy[df_copy['extra'] == 83])\n",
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows where passenger count is 83\n",
    "#df_copy = remove_unwanted_values(df_copy, \"extra\", 83)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_values = [0.5, 0.0, 1 , -0.5 , -1]\n",
    "df_copy = adjust_column_for_multiple_values(df_copy, 'extra', 'total_amount', valid_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_copy[\"extra\"].unique()\n",
    "print(unique_values)\n",
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We simply fixed the row with `Extra` value of 83 from the dataset since this is clearly an outlier since we know from the column description that Extra could only be 0.5 or 1 (possibly 0), however this value is way too far off which indicates data entry error. we zerod that value and fixed the total amount accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['trip_distance'].skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_density(df_copy, 'trip_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1_distance = df_copy.trip_distance.quantile(0.25)\n",
    "Q3_distance = df_copy.trip_distance.quantile(0.75)\n",
    "IQR_distance = Q3_distance - Q1_distance\n",
    "print(IQR_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off_distance = IQR_distance * 1.5\n",
    "lower_distance = Q1_distance - cut_off_distance\n",
    "upper_distance =  Q3_distance + cut_off_distance\n",
    "print(lower_distance,upper_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_copy[df_copy['trip_distance'] > upper_distance]\n",
    "df2 = df_copy[df_copy['trip_distance'] < lower_distance]\n",
    "print('Total number of outliers are', df1.shape[0]+ df2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.trip_distance.quantile(0.93)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to The fact that the data is very skewed thus it's not normally distrubuted we can't use Zscore, thus to detect outliers I used IQR, and mean imputed the outliers using the mean of the data that falls in IQR range, since the outliers will affect the mean if we calculate the whole data mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = impute_outliers_with_mean_iqr(df_copy, 'trip_distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['trip_distance'].skew()\n",
    "df_copy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['trip_distance'].skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Max in the trip distance is way lower now and reasonable , moreover the skewness is better now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(df_copy['fare_amount'].skew())\n",
    "display(df_copy['tip_amount'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density(df_copy, 'fare_amount')\n",
    "plot_density(df_copy, 'tip_amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The same concept applies for the `fare_amount` and `tip_amount` , we can't apply zscore since the data is very skewed and not normally distrubted, thus we apply IQR and mean impute the outliers using the mean of the data that falls in IQR range, since the outliers will affect the mean if we calculate the whole data mean. Moreover, we need to fix the `total_ amount` column for the values we imputed for it to keep the data consisteny ( all components aligning with the `total_amount` ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = impute_outliers_with_mean_iqr(df_copy, 'fare_amount', 'total_amount')\n",
    "df_copy = impute_outliers_with_mean_iqr(df_copy, 'tip_amount', 'total_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df_copy['fare_amount'].skew())\n",
    "display(df_copy['tip_amount'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density(df_copy, 'tolls_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy['tolls_amount'].skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['tolls_amount'].quantile(0.90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_copy['tolls_amount']<0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(df_copy['tolls_amount']>0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_copy['tolls_amount']>8).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_copy['tolls_amount']>15).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_copy['tolls_amount']>30).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = impute_outliers_with_mean(df_copy, 'tolls_amount','total_amount', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the `tolls_amount` we can clearly see that the data is very skewed, thus we can't apply z-score as well, however here we can't apply IQR as well since the first Quartile and the third Quartile are both zeros, which will give an IQR a value of zero, thus we need to impute the values above a threshold.  However, for this dataset, it's crucial to compute the mean only from rides that incurred tolls, so as not to skew the data. Furthermore, to prevent extreme outliers from heavily influencing the mean (like a toll of $900), we'll set a reasonable threshold. We'll then compute the mean from non-zero, positive toll amounts below this threshold. Values exceeding this threshold will be imputed using the calculated mean. We should also account for the change in the value that should be reflected in the `total_amount` feature. The Threshold used here is reasonable for a ride with toll_amount based on the data we saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear after we handled the tolls outliers we still kept the data consistent.\n",
    "count_mismatch = count_mismatch_rows(df_copy, ['extra', 'mta_tax', 'tolls_amount','improvement_surcharge','fare_amount','tip_amount'], 'total_amount')\n",
    "display(count_mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(df_copy['tolls_amount'].skew())\n",
    "display(df_copy['total_amount'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the `tolls_amount` the data is still skewed due to the fact that 90 percent of rides didnt incurr tolls, but the only way to handle the outliers and make the toll amount was through the method discussed above, moreover the data is now reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_copy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the rest of the columns they have discrete values and inorder to apply statistical analysis, the data needs to be continous. Moreover we know that they are fine since we already handled the incorrect 'discrete' values earlier so the data is already correct and we are sure about that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Feature\"></a>\n",
    "# 4 - Data transformation and feature eng."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this section we will discretize all columns that are continous using equal width discretization and added the 2 new columns `week_number` , and `date_range`. Equal width discretization was used for its Consistency, since this method doesn't depend on the underlying data distribution. This means that if you have a predetermined set of bins, new data can be consistently categorized into these bins even if the distribution of the new data is different from the old. Moreover Uniformity since every bin has the same width. This can be useful when you need to categorize data into fixed ranges regardless of the distribution of data within those ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "def add_weekly_columns(df, datetime_column):\n",
    "    \"\"\"\n",
    "    Add 'week_number' and 'data_range' column to the dataframe based on a datetime column.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The dataframe\n",
    "    - datetime_column: The column in df containing datetime values\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with two new columns 'week_number' and 'date_range'\n",
    "    \"\"\"\n",
    "    \n",
    "    # Determine the start of the week for the earliest date in the dataframe\n",
    "    first_date = df[datetime_column].min()\n",
    "    start_of_first_week = first_date - pd.to_timedelta(first_date.dayofweek, unit='D')\n",
    "\n",
    "    # Compute the day difference to the start of the first week\n",
    "    day_difference = (df[datetime_column] - start_of_first_week).dt.days\n",
    "\n",
    "    # Use this difference to determine the week number\n",
    "    df['week_number'] = (day_difference // 7) + 1\n",
    "\n",
    "    week_start = (df[datetime_column] - pd.to_timedelta(df[datetime_column].dt.dayofweek, unit='D')).dt.date\n",
    "    week_end = (week_start + pd.DateOffset(days=6)).dt.date\n",
    "    df['week_range'] = week_start.astype(str) + ' / ' + week_end.astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def equal_width_discretization(df, column_name, bins=10, new_column_name=None, labels=None):\n",
    "    \"\"\"\n",
    "    Perform Equal Width Discretization on a specified column of a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The dataframe.\n",
    "    - column_name: Name of the column to be discretized.\n",
    "    - bins: Number of equal width bins to create (default is 10).\n",
    "    - new_column_name: Name of the new column to store the discretized data (default is None, which overwrites the existing column).\n",
    "    - labels: List of labels for the bins. If not provided, defaults to numerical labels.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with the discretized column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If no labels provided, use numerical bin labels\n",
    "    if labels is None:\n",
    "        labels = [i for i in range(1, bins + 1)]\n",
    "    elif len(labels) != bins:\n",
    "        raise ValueError(f\"Number of labels ({len(labels)}) should match the number of bins ({bins}).\")\n",
    "\n",
    "    # If no new column name provided, overwrite the original column\n",
    "    if new_column_name is None:\n",
    "        new_column_name = column_name\n",
    "    \n",
    "    # Check if the column is of datetime type\n",
    "    if df[column_name].dtype == 'datetime64[ns]':\n",
    "        min_date = df[column_name].min()\n",
    "        \n",
    "        # Creating bins with 7 day width using pd.to_timedelta\n",
    "        bin_edges = [min_date + pd.to_timedelta(7 * i, unit='D') for i in range(bins + 1)]\n",
    "        series_to_cut = df[column_name]\n",
    "    else:\n",
    "        series_to_cut = df[column_name]\n",
    "        bin_edges = bins\n",
    "    \n",
    "    # Use pandas cut function to create the discretized column\n",
    "    df[new_column_name] = pd.cut(series_to_cut, bins=bin_edges, labels=labels, include_lowest=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def equal_width_discretization_range(df, column_name, bins=10, new_column_name=None):\n",
    "    \"\"\"\n",
    "    Perform Equal Width Discretization on a specified column of a DataFrame and return the date ranges for each bin.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: The dataframe.\n",
    "    - column_name: Name of the column to be discretized.\n",
    "    - bins: Number of equal width bins to create (default is 10).\n",
    "    - new_column_name: Name of the new column to store the date range (default is None, which overwrites the existing column).\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with the date range column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # If no new column name provided, overwrite the original column\n",
    "    if new_column_name is None:\n",
    "        new_column_name = column_name\n",
    "    \n",
    "    # Check if the column is of datetime type\n",
    "    if df[column_name].dtype == 'datetime64[ns]':\n",
    "        min_date = df[column_name].min()\n",
    "        \n",
    "        # Creating bins with 7 day width using pd.to_timedelta\n",
    "        bin_edges = [min_date + pd.to_timedelta(7 * i, unit='D') for i in range(bins + 1)]\n",
    "        series_to_cut = df[column_name]\n",
    "    else:\n",
    "        series_to_cut = df[column_name]\n",
    "        bin_edges = bins\n",
    "    \n",
    "    # Generate labels for the date ranges\n",
    "    labels = [f\"{bin_edges[i].strftime('%Y-%m-%d')} / {(bin_edges[i+1] - pd.to_timedelta(1, unit='D')).strftime('%Y-%m-%d')}\" for i in range(len(bin_edges)-1)]\n",
    "    \n",
    "    # Use pandas cut function to create the discretized column\n",
    "    df[new_column_name] = pd.cut(series_to_cut, bins=bin_edges, labels=labels, include_lowest=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = equal_width_discretization(df_copy, \"trip_distance\", bins=5, new_column_name=\"trip_distance_discretized\", labels=[\"low\", \"medium-low\", \"medium\", \"medium-high\", \"high\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(df_copy.tail(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = equal_width_discretization(df_copy, \"fare_amount\", bins=5, new_column_name=\"fare_amount_discretized\", labels =[\"low\", \"medium-low\", \"medium\", \"medium-high\", \"high\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df_copy.tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = equal_width_discretization(df_copy, \"tip_amount\", bins=3, new_column_name=\"tip_amount_discretized\", labels =[\"low\", \"medium\",\"high\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = equal_width_discretization(df_copy, \"total_amount\", bins=5, new_column_name=\"total_amount_discretized\", labels =[\"low\", \"medium-low\", \"medium\", \"medium-high\", \"high\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_copy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_copy = equal_width_discretization(df_copy, \"pickup_datetime\", bins=5, new_column_name=\"week_number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = df_copy[\"week_number\"].value_counts()\n",
    "print(unique_values)\n",
    "df_copy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_df = df_copy[df_copy['pickup_datetime'].dt.day == 7]\n",
    "\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "unique_values = df_copy[\"week_number\"].unique()\n",
    "print(unique_values)\n",
    "df_copy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = equal_width_discretization_range(df_copy, \"pickup_datetime\", bins=5, new_column_name=\"date_range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df_copy[df_copy['pickup_datetime'].dt.day == 7]\n",
    "\n",
    "display(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df_copy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As shown this makes sense, since we have 5 weeks in January ( week 5 accounts for the last couple of days in January ) and the date range within every week. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"GPS\"></a>\n",
    "## 4.2 - Additional data extraction (GPS coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from geopy.geocoders import GoogleV3\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(city_name, geolocator):\n",
    "    location = geolocator.geocode(city_name)\n",
    "    if location:\n",
    "        return location.latitude, location.longitude\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "def get_coordinates_google(location, api_key):\n",
    "    geolocator = GoogleV3(api_key=api_key)\n",
    "\n",
    "    # Split location into borough and zone\n",
    "    parts = location.split(', ')\n",
    "    full_location = parts[0] + \", \" + parts[1]\n",
    "    \n",
    "    try:\n",
    "        # Try to geocode using full location (borough, zone)\n",
    "        location_obj = geolocator.geocode(full_location, timeout=10)\n",
    "        if location_obj:\n",
    "            return location_obj.latitude, location_obj.longitude\n",
    "        \n",
    "        # If that fails, try just the borough\n",
    "        location_obj = geolocator.geocode(parts[0], timeout=10)\n",
    "        if location_obj:\n",
    "            return location_obj.latitude, location_obj.longitude\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching coordinates for {location}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "    time.sleep(1)  # To prevent hitting request limits\n",
    "    return None, None\n",
    "    \n",
    "def gather_and_save_unique_coordinates(df, api_key, pu_column='pickup_location', do_column='dropoff_location', filename=\"all_location_coordinates.csv\"):\n",
    "    \"\"\"\n",
    "    Gathers unique GPS coordinates for the given pickup and drop-off columns and saves them to a CSV.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract unique values from both columns\n",
    "    unique_pu_locations = df[pu_column].unique()\n",
    "    unique_do_locations = df[do_column].unique()\n",
    "\n",
    "    # Combine and deduplicate\n",
    "    all_unique_locations = set(unique_pu_locations) | set(unique_do_locations)\n",
    "\n",
    "    # If CSV doesn't exist, fetch coordinates and save to CSV\n",
    "    if not os.path.exists(filename):\n",
    "        coordinates = {}\n",
    "        for location in all_unique_locations:\n",
    "            coords = get_coordinates_google(location, api_key)\n",
    "            if coords:\n",
    "                coordinates[location] = coords\n",
    "        # Save to CSV\n",
    "        coordinates_df = pd.DataFrame.from_dict(coordinates, orient='index', columns=['Latitude', 'Longitude'])\n",
    "        coordinates_df.to_csv(filename)\n",
    "    else:\n",
    "        coordinates_df = pd.read_csv(filename, index_col=0)\n",
    "    \n",
    "    return coordinates_df\n",
    "\n",
    "def populate_lat_long(df, prefix, coordinates_df):\n",
    "    df[prefix + '_lat'] = df[prefix + '_location'].map(coordinates_df['Latitude'])\n",
    "    df[prefix + '_long'] = df[prefix + '_location'].map(coordinates_df['Longitude'])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "api_key = \"AIzaSyBXV_Q4_CWvV7btH9drTwc3BYRoj2GwozQ\"\n",
    "coordinates_df = gather_and_save_unique_coordinates(df_copy, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populating the columns\n",
    "df_copy = populate_lat_long(df_copy, 'pickup', coordinates_df)\n",
    "df_copy = populate_lat_long(df_copy, 'dropoff', coordinates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coordinates_df = pd.read_csv(\"all_location_coordinates.csv\")\n",
    "display(coordinates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_copy.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy[[\"pickup_lat\",\"dropoff_lat\"]].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Those are the cells that the api didnt find coordinates for, Moreover this is due to the fact that the borough, zone are both unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 - Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### those are the categorical features,  `vendor`, `store_and_fwd` , `trip_type`, `rate_type`,`pickup_location`,`dropoff_location`,  and `payment_type` that could be encoded:\n",
    "- We can encode `vendor`, `store_and_fwd`,`payment_type`, `rate_type` and `trip_type` using One_Hot encoding.\n",
    "- we can encode ,`pickup_location`and `dropoff_location` using label encoding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reasoning, for the `vendor`, `store_and_fwd` and `trip_type`, One-hot encoding is appropriate. Even though they are binary, if there's no clear ordinal relationship between the two values, using label encoding might impose an unintended ordinality. For example, assigning 0 and 1 might make certain algorithms treat them as having some kind of rank. Since they don't increase dimensionality by much (just an extra column), the trade-off is minimal, Moreover we can remove first thus we don't increase the columns (which in the end gives the same scenario as label encoding), so the decision here doesn't really matter. However for the `rate_type` and `payment_type` they don't have ordinal relation between them, thus if we use one-hot this will increase the feature space by 10 (they have 10 categories combined) which will increase the size of the dataset, and if we use label encoding we might inforce some kind of rank between them. Therefore, it's a tradeoff here. I prefer using one hot encoding for it since it will be more readable and it will avoid model confusion by not inducing ranking among the categories, howerver it will increase the feature space. I also didn't use one-hot encoding of the most frequent categories since I'll lose information about the other less frequent categories. Futhermore for the pu_location and do_location since they have a lot of unique values , doing them using one hot will cause a huge increase in the dimensions of the data will add so many columns this way, thus it's more appropriote to use label encoding for the data preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "def encode_features(df, features, method):\n",
    "    \"\"\"\n",
    "    Encode given columns of a dataframe.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to be encoded.\n",
    "    - features: List of column names to be encoded.\n",
    "    - method: Encoding method - either 'onehot' or 'label'.\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with encoded features.\n",
    "    - Dictionary of mappings from original to encoded values for each feature.\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    mappings = {}\n",
    "\n",
    "    # One-Hot Encoding\n",
    "    if method == 'onehot':\n",
    "        one_hot_encoder = OneHotEncoder(drop='first', sparse_output=False)\n",
    "        \n",
    "        df_encoded = pd.DataFrame(one_hot_encoder.fit_transform(df_copy[features]), \n",
    "                                  columns=one_hot_encoder.get_feature_names_out(features),\n",
    "                                  index=df_copy.index)  # Important: Maintain the original index\n",
    "        \n",
    "        # Mapping for one-hot encoding - for each feature, a dict of category to encoded column\n",
    "        for feature, cats in zip(features, one_hot_encoder.categories_):\n",
    "            mappings[feature] = {cat: f\"{feature}_{cat}\" for cat in cats[1:]}  # Exclude first category\n",
    "        \n",
    "        df_copy = pd.concat([df_copy, df_encoded], axis=1)\n",
    "        df_copy.drop(features, axis=1, inplace=True)  # Remove original columns after encoding\n",
    "    \n",
    "    # Label Encoding\n",
    "    elif method == 'label':\n",
    "        for feature in features:\n",
    "            label_encoder = LabelEncoder()\n",
    "            df_copy[feature] = label_encoder.fit_transform(df_copy[feature])  # In-place modification\n",
    "            mappings[feature] = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))\n",
    "    else:\n",
    "        raise ValueError(\"Method must be either 'onehot' or 'label'\")\n",
    "    \n",
    "    return df_copy, mappings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "onehot_features = ['vendor', 'store_and_fwd', 'payment_type', 'rate_type', 'trip_type']\n",
    "label_features = ['pickup_location','dropoff_location']\n",
    "\n",
    "df_copy, onehot_mappings = encode_features(df_copy, onehot_features, 'onehot')\n",
    "\n",
    "label_features = ['pickup_location','dropoff_location']\n",
    "df_copy, label_mappings = encode_features(df_copy, label_features, 'label')\n",
    "update_lookup(label_mappings)\n",
    "\n",
    "display(lookup_table)\n",
    "\n",
    "display(df_copy.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy[label_features].tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As shown, the data has been encoded for the onehot columns only a single column for the binary features was added since we used drop first, we can induce which type based on the value of that feature using 1 column and for the `rate_type` and `payment_type` a new column was added for each category except the first one since we use drop first as well. Moreover for the `pickup_location` and `dropoff_location` they were encoded using label-encoding and it could be seen in the previous cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 - Normalisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we don't know what is the machine learning model that will be used, thus we shouldn't normalise the data and change its distribution, moreover we already know that the columns aren't skewed after we handled the outliers earlier, however the `fare_amount` has a high STD consequently the `total_amount` too, which could benefit from normalisation,but it will change the distribution and we are tampering with financial information which is wrong in that context and again we don't know what machine learning model that will be used, thus normalising isn't beneficial in that case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 - Adding more features(feature eng.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Add 3 new features: \n",
    " - Duration: Difference between dropoff_datetime and pickup_datetime.\n",
    " - Weekend: Whether the trip started on a weekend.\n",
    " - Average Speed: trip_distance divided by Duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    \n",
    "    # 1. Calculate Duration in hours\n",
    "    df['duration_hours'] = (df['dropoff_datetime'] - df['pickup_datetime']).dt.total_seconds() / 3600\n",
    "    \n",
    "    # 2. Identify if the trip was on a weekend\n",
    "    df['is_weekend'] = df['pickup_datetime'].dt.weekday >= 5  # 5 for Saturday, 6 for Sunday\n",
    "    \n",
    "    # 3. Calculate Average Speed in miles per hour\n",
    "    df['avg_speed_mph'] = df['trip_distance'] / df['duration_hours']\n",
    "    # Set avg_speed_mph to -1 where trip_distance is -1\n",
    "    df['avg_speed_mph'] = np.where(df['trip_distance'] == -1, -1, df['avg_speed_mph'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = add_features(df_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(df_copy.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As shown the 3 new features were added with their respective values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_mismatch = count_mismatch_rows(df_copy, ['extra', 'mta_tax', 'tolls_amount','improvement_surcharge','fare_amount','tip_amount'], 'total_amount')\n",
    "display(count_mismatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_copy.columns)\n",
    "display(null_percentage(df_copy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"LookUP\"></a>\n",
    "## 4.6 - Csv file for lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = []\n",
    "\n",
    "for feature, mapping in lookup_table.items():\n",
    "    for original_value, encoded_value in mapping.items():\n",
    "        flattened_data.append({\n",
    "            'Column Name': feature,\n",
    "            'Original Value': original_value,\n",
    "            'Imputed/Encoded Value': encoded_value\n",
    "        })\n",
    "\n",
    "lookup_df = pd.DataFrame(flattened_data)\n",
    "\n",
    "lookup_df.to_csv('lookup_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"Parquet\"></a>\n",
    "## 5- Exporting the dataframe to a csv file or parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace df to df final\n",
    "df_copy.to_csv('green_tripdata_16-01_clean.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy.to_parquet('green_trip_data_16-1_clean.parquet', engine='pyarrow', compression='snappy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
